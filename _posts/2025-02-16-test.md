---
layout: distill
title: Masked Language Model with ALiBi and CLAP head
description: As a new approach to positional encoding, Attention with Linear Biases (ALiBi) uses linear biases of the attention weights to encode positional information, with capability of context length extrapolation. In their paper however, Press et al. focus on the perplexity of autoregressive decoder-only language models, leaving the question of downstream tasks and its applicability to encoder-attention open. In this blogpost, we attempt to bridge the gap by testing masked language models (MLMs) with encoder-attention ALiBi and prediction head similar to the counterparts of the original ALiBi models. We find that while simplified prediction head may be beneficial, performance of MLMs with encoder-attention ALiBi starts to deteriorate with 2048 sequence length at larger scales. We put our results in the context of related recent experiments and tentatively identify the circumstances more challenging to positional encoding designs. Finally, we open-source our MLMs, with BERT-level performance and 2048 context length.
date: 2025-02-16
future: true
htmlwidgets: true

authors:
  - name: Jason Chuan-Chih Chou
    url: https://scholar.google.com/citations?user=V7BXGawAAAAJ
    affiliations:
      name: Cohere For AI Community

# must be the exact same name as your blogpost
bibliography: 2024-05-07-alibi-mlm.bib


This message is used to verify that this feed (feedId:113856371086613504) belongs to me (userId:112761546605710336). Join me in enjoying the next generation information browser https://follow.is.
